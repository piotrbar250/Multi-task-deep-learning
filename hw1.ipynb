{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "077df0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
    "# !unzip data_gsn.zip &> /dev/null\n",
    "# !rm data_gsn.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f3cd8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa7e8d3a490>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9c13152",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAIRS = [(i, j) for i in range(6) for j in range(i + 1, 6)]  # 15 unordered pairs\n",
    "PAIR_TO_IDX = {p: k for k, p in enumerate(PAIRS)}\n",
    "N_CONFIGS = len(PAIRS) * 9  # 135\n",
    "\n",
    "def class_id_to_pair_and_split(class_id: int):\n",
    "    pair_idx = class_id // 9\n",
    "    split_idx = class_id % 9  # 0..8 -> counts 1..9\n",
    "    ca = split_idx + 1\n",
    "    cb = 10 - ca\n",
    "    i, j = PAIRS[pair_idx]\n",
    "    return (i, j), (ca, cb)\n",
    "\n",
    "def class_id_to_pair(class_id: int):\n",
    "    pair_idx = class_id // 9\n",
    "    return PAIRS[pair_idx]\n",
    "\n",
    "def counts_to_class_id(counts):\n",
    "    if isinstance(counts, torch.Tensor):\n",
    "        c = counts.detach().cpu().tolist()\n",
    "    else:\n",
    "        c = list(counts)\n",
    "\n",
    "    nz = [i for i, v in enumerate(c) if v > 0]\n",
    "    if len(nz) != 2:\n",
    "        raise ValueError(f\"Expected exactly 2 nonzero counts, got {len(nz)}: {c}\")\n",
    "    if sum(c) != 10:\n",
    "        raise ValueError(f\"Expected counts to sum to 10, got {sum(c)}: {c}\")\n",
    "\n",
    "    a, b = sorted(nz)\n",
    "    ca = int(c[a])\n",
    "    pair_index = PAIR_TO_IDX[(a, b)]\n",
    "\n",
    "    class_id = pair_index * 9 + (ca - 1)\n",
    "    return class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02cf7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSN(Dataset):\n",
    "    def __init__(self, root, transform=None, transform_relabel=None):\n",
    "        self.data_dir = os.path.join(root, \"data\")\n",
    "        self.transform = transform\n",
    "        self.transform_relabel = transform_relabel\n",
    "\n",
    "        df = pd.read_csv(os.path.join(self.data_dir, \"labels.csv\"))\n",
    "        self.names = df[\"name\"].tolist()\n",
    "        cols = [\"squares\", \"circles\", \"up\", \"right\", \"down\", \"left\"]\n",
    "        self.labels = torch.tensor(df[cols].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        name = self.names[index]\n",
    "        img_path = os.path.join(self.data_dir, name)\n",
    "\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        img = transforms.ToTensor()(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        cnt = self.labels[index]\n",
    "\n",
    "        if self.transform_relabel:\n",
    "            img, cnt = self.transform_relabel(img, cnt)\n",
    "        \n",
    "        cls = counts_to_class_id(cnt)\n",
    "\n",
    "        return img, cls, cnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5db8493",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmentation:\n",
    "    def __init__(self, p_hflip=0.5, p_vflip=0.5):\n",
    "        self.p_hflip = p_hflip\n",
    "        self.p_vflip = p_vflip\n",
    "\n",
    "    def __call__(self, img, cnt):\n",
    "        cnt = cnt.clone()\n",
    "\n",
    "        k = torch.randint(0, 4, (1,)).item()\n",
    "        if k > 0:\n",
    "            img = torch.rot90(img, k=-k, dims=[1,2])\n",
    "            dirs = cnt[2:6]\n",
    "            dirs = torch.roll(dirs, shifts=k)\n",
    "            cnt[2:6] = dirs\n",
    "\n",
    "        if torch.rand(1).item() < self.p_hflip:\n",
    "            img = torch.flip(img, dims=[2])\n",
    "            cnt[[3, 5]] = cnt[[5, 3]]\n",
    "        \n",
    "        if torch.rand(1).item() < self.p_vflip:\n",
    "            img = torch.flip(img, dims=[1])\n",
    "            cnt[[2, 4]] = cnt[[4, 2]]\n",
    "\n",
    "        return img, cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6103b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, cls_hidden=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = nn.Sequential(                              # (64, 1, 28, 28)\n",
    "            nn.Conv2d(1, 8, 3, stride=1, padding=1), nn.ReLU(),     # (64, 8, 28 28)  \n",
    "            nn.Conv2d(8, 16, 3, stride=1, padding=1), nn.ReLU(),    # (64, 16, 28, 28)\n",
    "            nn.Conv2d(16, 32, 3, stride=1, padding=1), nn.ReLU(),   # (64, 32, 28, 28)\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1), nn.ReLU(),   # (64, 64, 28, 28)\n",
    "            nn.Flatten(start_dim=1),                                # (64, 64 * 28 * 28)\n",
    "            nn.Linear(64 * 28 * 28, 256), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.head_cls = nn.Sequential(\n",
    "            nn.Linear(256, cls_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(cls_hidden, 135),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.head_cnt = nn.Sequential(\n",
    "            nn.Linear(256, 6)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        cls = self.head_cls(x)  # (64, 135)\n",
    "        cnt = self.head_cnt(x)  # (64, 6)\n",
    "\n",
    "        return cls, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc4775f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    net: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    log_interval: int,\n",
    "    mode: str,\n",
    "    lambda_cnt = None,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    net.train()\n",
    "    total_loss = total_cls_loss = total_cnt_loss = 0.0\n",
    "    n_total = 0\n",
    "    \n",
    "    for batch_idx, (img, cls_target, cnt_target) in enumerate(train_loader):\n",
    "        img, cls_target, cnt_target = img.to(device), cls_target.long().to(device), cnt_target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cls_pred, cnt_pred = net(img)\n",
    "        \n",
    "        cls_loss = F.nll_loss(cls_pred, cls_target)\n",
    "        cnt_loss = F.smooth_l1_loss(cnt_pred, cnt_target)\n",
    "\n",
    "        if mode == \"cls_only\":\n",
    "            loss = cls_loss\n",
    "        elif mode == \"reg_only\":\n",
    "            loss = cnt_loss\n",
    "        else:\n",
    "            loss = cls_loss + lambda_cnt * cnt_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        B = len(img)\n",
    "\n",
    "        total_loss += loss.item() * B\n",
    "        total_cls_loss += cls_loss.item() * B\n",
    "        total_cnt_loss += cnt_loss.item() * B\n",
    "        n_total += B\n",
    "\n",
    "        if verbose and batch_idx % log_interval == 0:\n",
    "            done = batch_idx * B\n",
    "            total = len(train_loader.dataset)\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch} [{done}/{total} images ({done / total:.0%})]\\t\"\n",
    "                + f\"Loss: {loss.item():.6f}\"\n",
    "            )\n",
    "\n",
    "    epoch_loss = total_loss / n_total\n",
    "    epoch_cls_loss = total_cls_loss / n_total\n",
    "    epoch_cnt_loss = total_cnt_loss / n_total\n",
    "    return epoch_loss, epoch_cls_loss, epoch_cnt_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "437bc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(\n",
    "    net: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    epoch: int,\n",
    "    mode: str,\n",
    "    lambda_cnt = None,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    net.eval()\n",
    "    total_loss = total_cls_loss = total_cnt_loss = 0.0\n",
    "    n_total = 0\n",
    "    correct = 0\n",
    "\n",
    "    sum_sq_diff = 0.0\n",
    "    n_cnt = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, cls_target, cnt_target in test_loader:\n",
    "            img, cls_target, cnt_target = img.to(device), cls_target.long().to(device), cnt_target.to(device)\n",
    "\n",
    "            cls_pred, cnt_pred = net(img)\n",
    "\n",
    "            cls_loss = F.nll_loss(cls_pred, cls_target)\n",
    "            cnt_loss = F.smooth_l1_loss(cnt_pred, cnt_target)\n",
    "\n",
    "            if mode == \"cls_only\":\n",
    "                loss = cls_loss\n",
    "            elif mode == \"reg_only\":\n",
    "                loss = cnt_loss\n",
    "            else:\n",
    "                loss = cls_loss + lambda_cnt * cnt_loss\n",
    "\n",
    "            total_loss += loss.item() * len(img)\n",
    "            total_cls_loss += cls_loss.item() * len(img)\n",
    "            total_cnt_loss += cnt_loss.item() * len(img)\n",
    "            n_total += len(img)\n",
    "\n",
    "            pred = cls_pred.argmax(dim=1)\n",
    "            correct += (pred == cls_target).sum().item()\n",
    "\n",
    "            diff = cnt_pred - cnt_target\n",
    "            sum_sq_diff += (diff ** 2).sum().item()\n",
    "            n_cnt += diff.numel()\n",
    "\n",
    "    epoch_loss = total_loss / n_total\n",
    "    epoch_cls_loss = total_cls_loss / n_total\n",
    "    epoch_cnt_loss = total_cnt_loss / n_total\n",
    "    epoch_acc = correct / n_total\n",
    "\n",
    "    eval_rmse = (sum_sq_diff / n_cnt) ** 0.5\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Eval Epoch: {epoch} | \"\n",
    "            f\"acc: {epoch_acc:.4f} | \"\n",
    "            f\"loss: {epoch_loss:.4f} | \"\n",
    "            f\"cls_loss: {epoch_cls_loss:.4f} | \"\n",
    "            f\"cnt_loss: {epoch_cnt_loss:.4f}\"\n",
    "        )\n",
    "    return epoch_loss, epoch_cls_loss, epoch_cnt_loss, epoch_acc, eval_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea7dab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(net, device, loader):\n",
    "    net.eval()\n",
    "\n",
    "    all_cls_true = []\n",
    "    all_cls_pred = []\n",
    "    all_cnt_true = []\n",
    "    all_cnt_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, cls_target, cnt_target in loader:\n",
    "            img = img.to(device)\n",
    "\n",
    "            cls_logits, cnt_pred = net(img)\n",
    "            cls_pred = cls_logits.argmax(dim=1)\n",
    "\n",
    "            all_cls_true.append(cls_target)\n",
    "            all_cls_pred.append(cls_pred.to(\"cpu\"))            \n",
    "            all_cnt_true.append(cnt_target)\n",
    "            all_cnt_pred.append(cnt_pred.to(\"cpu\"))\n",
    "\n",
    "    cls_true = torch.cat(all_cls_true)\n",
    "    cls_pred = torch.cat(all_cls_pred)\n",
    "    cnt_true = torch.cat(all_cnt_true)\n",
    "    cnt_pred = torch.cat(all_cnt_pred)\n",
    "\n",
    "    acc = (cls_true == cls_pred).float().mean().item()\n",
    "    print(f\"Top-1 accuracy: {acc}\" )  \n",
    "\n",
    "    cls_true_np = cls_true.numpy() \n",
    "    cls_pred_np = cls_pred.numpy() \n",
    "    macro_f1 = f1_score(cls_true_np, cls_pred_np, average=\"macro\")\n",
    "    print(f\"Macro F1:{macro_f1}\")\n",
    "\n",
    "    correct_pair = 0\n",
    "    total = len(cls_true)\n",
    "    for t, p in zip(cls_true.tolist(), cls_pred.tolist()):\n",
    "        if class_id_to_pair(int(t)) == class_id_to_pair(int(p)):\n",
    "            correct_pair += 1\n",
    "    pair_acc = correct_pair / total\n",
    "    print(f\"Per-pair accuracy: {pair_acc}\")\n",
    "\n",
    "    diff = cnt_pred - cnt_true\n",
    "    mse_per_class = (diff ** 2).mean(dim=0)\n",
    "    rmse_per_class = torch.sqrt(mse_per_class)\n",
    "    mae_per_class = diff.abs().mean(dim=0)\n",
    "    overall_rmse = torch.sqrt((diff ** 2).mean()).item()\n",
    "    overall_mae = diff.abs().mean().item()  \n",
    "    print(f\"RMSE per class: {rmse_per_class.tolist()}\")\n",
    "    print(f\"MAE per class: {mae_per_class.tolist()}\")\n",
    "    print(f\"Overall RMSE: {overall_rmse}\")\n",
    "    print(f\"Overall MAE: {overall_mae}\")\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"pair_acc\": pair_acc,\n",
    "        \"rmse_per_class\": rmse_per_class.tolist(),\n",
    "        \"mae_per_class\": mae_per_class.tolist(),\n",
    "        \"overall_rmse\": overall_rmse,\n",
    "        \"overall_mae\": overall_mae,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baf72c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(root=\".\", batch_size=64, test_batch_size=1000, device=torch.device(\"cpu\")):\n",
    "    if device.type == \"cuda\":\n",
    "        num_workers = min(8, os.cpu_count() or 2)\n",
    "    else:\n",
    "        num_workers = 0\n",
    "\n",
    "    pin = (device.type == \"cuda\")\n",
    "    loader_kwargs = dict(\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    if num_workers > 0:\n",
    "        loader_kwargs[\"prefetch_factor\"] = 4\n",
    "\n",
    "    train_aug = Augmentation()\n",
    "    train_full = GSN(root=root, transform_relabel=train_aug)\n",
    "    test_full = GSN(root=root)\n",
    "\n",
    "    train_dataset = Subset(train_full, range(0, 9000))\n",
    "    test_dataset = Subset(test_full, range(9000, 10000))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **loader_kwargs)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, **loader_kwargs)\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9897ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 1e-3\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "\n",
    "cls_hidden = 256\n",
    "dropout = 0.3\n",
    "patience = 3\n",
    "\n",
    "def train_model(\n",
    "    mode: str,\n",
    "    lambda_cnt = None,\n",
    "    log_interval: int = 10,\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    train_loader, test_loader = create_loaders(\n",
    "        root=\".\",\n",
    "        batch_size=64,\n",
    "        test_batch_size=1000,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    net = NeuralNetwork(cls_hidden, dropout).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_cls_loss\": [],\n",
    "        \"train_cnt_loss\": [],\n",
    "        \"eval_loss\": [],\n",
    "        \"eval_cls_loss\": [],\n",
    "        \"eval_cnt_loss\": [],\n",
    "        \"eval_acc\": [],\n",
    "        \"eval_rmse\": [],\n",
    "    }\n",
    "\n",
    "    best_eval_loss = float(\"inf\")\n",
    "    best_acc = 0.0\n",
    "    best_state = None\n",
    "    bad_epochs = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss, train_cls_loss, train_cnt_loss = train_epoch(\n",
    "            net,\n",
    "            device,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            log_interval,\n",
    "            mode,\n",
    "            lambda_cnt=lambda_cnt,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        eval_loss, eval_cls_loss, eval_cnt_loss, eval_acc, eval_rmse = eval_epoch(\n",
    "            net,\n",
    "            device,\n",
    "            test_loader,\n",
    "            epoch,\n",
    "            mode,\n",
    "            lambda_cnt=lambda_cnt,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_cls_loss\"].append(train_cls_loss)\n",
    "        history[\"train_cnt_loss\"].append(train_cnt_loss)\n",
    "        history[\"eval_loss\"].append(eval_loss)\n",
    "        history[\"eval_cls_loss\"].append(eval_cls_loss)\n",
    "        history[\"eval_cnt_loss\"].append(eval_cnt_loss)\n",
    "        history[\"eval_acc\"].append(eval_acc)\n",
    "        history[\"eval_rmse\"].append(eval_rmse)\n",
    "\n",
    "        if eval_loss < best_eval_loss:\n",
    "            best_eval_loss = eval_loss\n",
    "            best_acc = eval_acc\n",
    "            best_state = {k: v.cpu().clone() for k,v in net.state_dict().items()}\n",
    "            bad_epochs = 0\n",
    "            best_epoch = epoch\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= patience:\n",
    "                print(\n",
    "                    f\"Early stop at epoch {epoch}. \"\n",
    "                    f\"Best val loss: {best_eval_loss:.4f}, \"\n",
    "                    f\"best acc: {best_acc:.4f}, \"\n",
    "                    f\"best epoch: {best_epoch}\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        net.load_state_dict(best_state)\n",
    "\n",
    "    metrics = evaluate_metrics(net, device, test_loader)\n",
    "\n",
    "    return net, history, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61403c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training mode=cls_only ===\n",
      "Eval Epoch: 1 | acc: 0.0110 | loss: 4.6770 | cls_loss: 4.6770 | cnt_loss: 1.4699\n",
      "Eval Epoch: 2 | acc: 0.0090 | loss: 4.6603 | cls_loss: 4.6603 | cnt_loss: 1.4704\n",
      "Eval Epoch: 3 | acc: 0.0260 | loss: 4.5862 | cls_loss: 4.5862 | cnt_loss: 1.4936\n",
      "Eval Epoch: 4 | acc: 0.0930 | loss: 3.2231 | cls_loss: 3.2231 | cnt_loss: 1.5757\n",
      "Eval Epoch: 5 | acc: 0.1930 | loss: 2.5722 | cls_loss: 2.5722 | cnt_loss: 1.6595\n",
      "Eval Epoch: 6 | acc: 0.2540 | loss: 2.2125 | cls_loss: 2.2125 | cnt_loss: 1.7657\n",
      "Eval Epoch: 7 | acc: 0.3240 | loss: 1.8823 | cls_loss: 1.8823 | cnt_loss: 1.7999\n",
      "Eval Epoch: 8 | acc: 0.3350 | loss: 1.7268 | cls_loss: 1.7268 | cnt_loss: 1.8856\n",
      "Eval Epoch: 9 | acc: 0.3680 | loss: 1.6133 | cls_loss: 1.6133 | cnt_loss: 1.9478\n",
      "Eval Epoch: 10 | acc: 0.3710 | loss: 1.5870 | cls_loss: 1.5870 | cnt_loss: 1.9187\n",
      "Top-1 accuracy: 0.3709999918937683\n",
      "Macro F1:0.31031759863389113\n",
      "Per-pair accuracy: 0.89\n",
      "RMSE per class: [3.1141912937164307, 3.096980094909668, 2.174018144607544, 2.81317138671875, 3.1910834312438965, 4.660772800445557]\n",
      "MAE per class: [2.233255386352539, 2.3390228748321533, 1.626573920249939, 2.5032031536102295, 2.0360538959503174, 3.4594521522521973]\n",
      "Overall RMSE: 3.2617881298065186\n",
      "Overall MAE: 2.366260290145874\n",
      "\n",
      "=== Training mode=reg_only ===\n",
      "Eval Epoch: 1 | acc: 0.0050 | loss: 1.2208 | cls_loss: 4.9210 | cnt_loss: 1.2208\n",
      "Eval Epoch: 2 | acc: 0.0110 | loss: 0.6359 | cls_loss: 4.9706 | cnt_loss: 0.6359\n",
      "Eval Epoch: 3 | acc: 0.0050 | loss: 0.3930 | cls_loss: 4.9663 | cnt_loss: 0.3930\n",
      "Eval Epoch: 4 | acc: 0.0110 | loss: 0.3913 | cls_loss: 4.9660 | cnt_loss: 0.3913\n",
      "Eval Epoch: 5 | acc: 0.0130 | loss: 0.3111 | cls_loss: 4.9695 | cnt_loss: 0.3111\n",
      "Eval Epoch: 6 | acc: 0.0100 | loss: 0.2817 | cls_loss: 4.9715 | cnt_loss: 0.2817\n",
      "Eval Epoch: 7 | acc: 0.0120 | loss: 0.2900 | cls_loss: 4.9687 | cnt_loss: 0.2900\n",
      "Eval Epoch: 8 | acc: 0.0100 | loss: 0.2627 | cls_loss: 4.9699 | cnt_loss: 0.2627\n",
      "Eval Epoch: 9 | acc: 0.0110 | loss: 0.2483 | cls_loss: 4.9702 | cnt_loss: 0.2483\n",
      "Eval Epoch: 10 | acc: 0.0140 | loss: 0.2426 | cls_loss: 4.9659 | cnt_loss: 0.2426\n",
      "Top-1 accuracy: 0.014000000432133675\n",
      "Macro F1:0.0016329535144828003\n",
      "Per-pair accuracy: 0.094\n",
      "RMSE per class: [0.6624475121498108, 0.792644739151001, 0.7545223236083984, 0.8038936853408813, 0.7428814768791199, 0.8585765361785889]\n",
      "MAE per class: [0.4524249732494354, 0.48629146814346313, 0.5218656063079834, 0.5023303031921387, 0.5002582669258118, 0.6112855672836304]\n",
      "Overall RMSE: 0.7715458273887634\n",
      "Overall MAE: 0.5124093890190125\n",
      "\n",
      "=== Training mode=multitask, lambda_cnt=1.0 ===\n",
      "Eval Epoch: 1 | acc: 0.0230 | loss: 5.5394 | cls_loss: 4.2655 | cnt_loss: 1.2739\n",
      "Eval Epoch: 2 | acc: 0.0510 | loss: 4.8347 | cls_loss: 3.7039 | cnt_loss: 1.1308\n",
      "Eval Epoch: 3 | acc: 0.1080 | loss: 4.0647 | cls_loss: 3.1007 | cnt_loss: 0.9640\n",
      "Eval Epoch: 4 | acc: 0.1760 | loss: 3.1577 | cls_loss: 2.5293 | cnt_loss: 0.6284\n",
      "Eval Epoch: 5 | acc: 0.2730 | loss: 2.3713 | cls_loss: 1.9553 | cnt_loss: 0.4160\n",
      "Eval Epoch: 6 | acc: 0.3110 | loss: 2.1165 | cls_loss: 1.7608 | cnt_loss: 0.3557\n",
      "Eval Epoch: 7 | acc: 0.3550 | loss: 1.9697 | cls_loss: 1.6479 | cnt_loss: 0.3218\n",
      "Eval Epoch: 8 | acc: 0.3600 | loss: 1.8837 | cls_loss: 1.5796 | cnt_loss: 0.3041\n",
      "Eval Epoch: 9 | acc: 0.3580 | loss: 1.8281 | cls_loss: 1.5327 | cnt_loss: 0.2954\n",
      "Eval Epoch: 10 | acc: 0.3630 | loss: 1.7837 | cls_loss: 1.5020 | cnt_loss: 0.2818\n",
      "Top-1 accuracy: 0.3630000054836273\n",
      "Macro F1:0.3142838304422005\n",
      "Per-pair accuracy: 0.916\n",
      "RMSE per class: [0.7341371178627014, 0.8109010457992554, 0.8362573385238647, 0.8413972854614258, 0.9025965332984924, 0.9104467034339905]\n",
      "MAE per class: [0.5081196427345276, 0.5519347190856934, 0.5531823039054871, 0.557213306427002, 0.6399063467979431, 0.5827710628509521]\n",
      "Overall RMSE: 0.8413666486740112\n",
      "Overall MAE: 0.5655211806297302\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "settings = [\n",
    "    (\"cls_only\", None),\n",
    "    (\"reg_only\", None),\n",
    "    (\"multitask\", 1.0),\n",
    "]\n",
    "\n",
    "for mode, lambda_cnt in settings:\n",
    "    if lambda_cnt is not None:\n",
    "        print(f\"\\n=== Training mode={mode}, lambda_cnt={lambda_cnt} ===\")\n",
    "        net, history, metrics = train_model(mode=mode, lambda_cnt=lambda_cnt)\n",
    "\n",
    "        results[(mode, lambda_cnt)] = {\n",
    "            \"history\": history,\n",
    "            \"metrics\": metrics,\n",
    "        }\n",
    "    else:\n",
    "        print(f\"\\n=== Training mode={mode} ===\")\n",
    "        net, history, metrics = train_model(mode=mode)\n",
    "\n",
    "        results[(mode)] = {\n",
    "            \"history\": history,\n",
    "            \"metrics\": metrics,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd01ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net, history = train_model(\n",
    "#     mode = \"multitask\",\n",
    "#     lambda_cnt=1.0\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0241b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = \"0535-net.pt\"\n",
    "# torch.save(net.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86ad3d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "# net = NeuralNetwork(cls_hidden, dropout).to(device)\n",
    "# net.load_state_dict(torch.load(\"0535-net.pt\"))\n",
    "\n",
    "# train_loader, test_loader = create_loaders(root=\".\", device=device)\n",
    "# metrics = evaluate_metrics(net, device, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
