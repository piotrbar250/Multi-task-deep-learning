{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "077df0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
    "# !unzip data_gsn.zip &> /dev/null\n",
    "# !rm data_gsn.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f3cd8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa94094e490>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9c13152",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAIRS = [(i, j) for i in range(6) for j in range(i + 1, 6)]  # 15 unordered pairs\n",
    "PAIR_TO_IDX = {p: k for k, p in enumerate(PAIRS)}\n",
    "N_CONFIGS = len(PAIRS) * 9  # 135\n",
    "\n",
    "def class_id_to_pair_and_split(class_id: int):\n",
    "    pair_idx = class_id // 9\n",
    "    split_idx = class_id % 9  # 0..8 -> counts 1..9\n",
    "    ca = split_idx + 1\n",
    "    cb = 10 - ca\n",
    "    i, j = PAIRS[pair_idx]\n",
    "    return (i, j), (ca, cb)\n",
    "\n",
    "def class_id_to_pair(class_id: int):\n",
    "    pair_idx = class_id // 9\n",
    "    return PAIRS[pair_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02cf7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSN(Dataset):\n",
    "    def __init__(self, root, transform=None, transform_relabel=None):\n",
    "        self.data_dir = os.path.join(root, \"data\")\n",
    "        self.transform = transform\n",
    "        self.transform_relabel = transform_relabel\n",
    "\n",
    "        df = pd.read_csv(os.path.join(self.data_dir, \"labels.csv\"))\n",
    "        self.names = df[\"name\"].tolist()\n",
    "        cols = [\"squares\", \"circles\", \"up\", \"right\", \"down\", \"left\"]\n",
    "        self.labels = torch.tensor(df[cols].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        name = self.names[index]\n",
    "        img_path = os.path.join(self.data_dir, name)\n",
    "\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        img = transforms.ToTensor()(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        cnt = self.labels[index]\n",
    "\n",
    "        if self.transform_relabel:\n",
    "            img, cnt = self.transform_relabel(img, cnt)\n",
    "        \n",
    "        cls = self.counts_to_class_id(cnt)\n",
    "\n",
    "        return img, cls, cnt\n",
    "    \n",
    "    def counts_to_class_id(self, counts):\n",
    "        if isinstance(counts, torch.Tensor):\n",
    "            c = counts.detach().cpu().tolist()\n",
    "        else:\n",
    "            c = list(counts)\n",
    "\n",
    "        nz = [i for i, v in enumerate(c) if v > 0]\n",
    "        if len(nz) != 2:\n",
    "            raise ValueError(f\"Expected exactly 2 nonzero counts, got {len(nz)}: {c}\")\n",
    "        if sum(c) != 10:\n",
    "            raise ValueError(f\"Expected counts to sum to 10, got {sum(c)}: {c}\")\n",
    "\n",
    "        a, b = sorted(nz)\n",
    "        ca = int(c[a])\n",
    "        pair_index = PAIR_TO_IDX[(a, b)]\n",
    "\n",
    "        class_id = pair_index * 9 + (ca - 1)\n",
    "        return class_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5db8493",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmentation:\n",
    "    def __init__(self, p_hflip=0.5, p_vflip=0.5):\n",
    "        self.p_hflip = p_hflip\n",
    "        self.p_vflip = p_vflip\n",
    "\n",
    "    def __call__(self, img, cnt):\n",
    "        cnt = cnt.clone()\n",
    "\n",
    "        k = torch.randint(0, 4, (1,)).item()\n",
    "        if k > 0:\n",
    "            img = torch.rot90(img, k=-k, dims=[1,2])\n",
    "            dirs = cnt[2:6]\n",
    "            dirs = torch.roll(dirs, shifts=k)\n",
    "            cnt[2:6] = dirs\n",
    "\n",
    "        if torch.rand(1).item() < self.p_hflip:\n",
    "            img = torch.flip(img, dims=[2])\n",
    "            cnt[[3, 5]] = cnt[[5, 3]]\n",
    "        \n",
    "        if torch.rand(1).item() < self.p_vflip:\n",
    "            img = torch.flip(img, dims=[1])\n",
    "            cnt[[2, 4]] = cnt[[4, 2]]\n",
    "\n",
    "        return img, cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6103b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, cls_hidden=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = nn.Sequential(                              # (64, 1, 28, 28)\n",
    "            nn.Conv2d(1, 8, 3, stride=1, padding=1), nn.ReLU(),     # (64, 8, 28 28)  \n",
    "            nn.Conv2d(8, 16, 3, stride=1, padding=1), nn.ReLU(),    # (64, 16, 28, 28)\n",
    "            nn.Conv2d(16, 32, 3, stride=1, padding=1), nn.ReLU(),   # (64, 32, 28, 28)\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1), nn.ReLU(),   # (64, 64, 28, 28)\n",
    "            nn.Flatten(start_dim=1),                                # (64, 64 * 28 * 28)\n",
    "            nn.Linear(64 * 28 * 28, 256), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.head_cls = nn.Sequential(\n",
    "            nn.Linear(256, cls_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(cls_hidden, 135),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.head_cnt = nn.Sequential(\n",
    "            nn.Linear(256, 6)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        cls = self.head_cls(x)  # (64, 135)\n",
    "        cnt = self.head_cnt(x)  # (64, 6)\n",
    "\n",
    "        return cls, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc4775f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    net: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    log_interval: int,\n",
    "    lambda_cnt: float,\n",
    "    mode: str,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    net.train()\n",
    "    total_loss = total_cls_loss = total_cnt_loss = 0.0\n",
    "    n_total = 0\n",
    "    \n",
    "    for batch_idx, (img, cls_target, cnt_target) in enumerate(train_loader):\n",
    "        img, cls_target, cnt_target = img.to(device), cls_target.long().to(device), cnt_target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cls_pred, cnt_pred = net(img)\n",
    "        \n",
    "        cls_loss = F.nll_loss(cls_pred, cls_target)\n",
    "        cnt_loss = F.smooth_l1_loss(cnt_pred, cnt_target)\n",
    "\n",
    "        if mode == \"cls_only\":\n",
    "            loss = cls_loss\n",
    "        elif mode == \"reg_only\":\n",
    "            loss = lambda_cnt * cnt_loss\n",
    "        else:\n",
    "            loss = cls_loss + lambda_cnt * cnt_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        B = len(img)\n",
    "\n",
    "        total_loss += loss.item() * B\n",
    "        total_cls_loss += cls_loss.item() * B\n",
    "        total_cnt_loss += cnt_loss.item() * B\n",
    "        n_total += B\n",
    "\n",
    "        if verbose and batch_idx % log_interval == 0:\n",
    "            done = batch_idx * B\n",
    "            total = len(train_loader.dataset)\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch} [{done}/{total} images ({done / total:.0%})]\\t\"\n",
    "                + f\"Loss: {loss.item():.6f}\"\n",
    "            )\n",
    "\n",
    "    epoch_loss = total_loss / n_total\n",
    "    epoch_cls_loss = total_cls_loss / n_total\n",
    "    epoch_cnt_loss = total_cnt_loss / n_total\n",
    "    return epoch_loss, epoch_cls_loss, epoch_cnt_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "437bc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(\n",
    "    net: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    epoch: int,\n",
    "    lambda_cnt: float,\n",
    "    mode: str,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    net.eval()\n",
    "    total_loss = total_cls_loss = total_cnt_loss = 0.0\n",
    "    n_total = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, cls_target, cnt_target in test_loader:\n",
    "            img, cls_target, cnt_target = img.to(device), cls_target.long().to(device), cnt_target.to(device)\n",
    "\n",
    "            cls_pred, cnt_pred = net(img)\n",
    "            cls_loss = F.nll_loss(cls_pred, cls_target)\n",
    "            cnt_loss = F.smooth_l1_loss(cnt_pred, cnt_target)\n",
    "\n",
    "            if mode == \"cls_only\":\n",
    "                loss = cls_loss\n",
    "            elif mode == \"reg_only\":\n",
    "                loss = lambda_cnt * cnt_loss\n",
    "            else:\n",
    "                loss = cls_loss + lambda_cnt * cnt_loss\n",
    "\n",
    "            total_loss += loss.item() * len(img)\n",
    "            total_cls_loss += cls_loss.item() * len(img)\n",
    "            total_cnt_loss += cnt_loss.item() * len(img)\n",
    "            n_total += len(img)\n",
    "\n",
    "            pred = cls_pred.argmax(dim=1)\n",
    "            correct += (pred == cls_target).sum().item()\n",
    "\n",
    "    epoch_loss = total_loss / n_total\n",
    "    epoch_cls_loss = total_cls_loss / n_total\n",
    "    epoch_cnt_loss = total_cnt_loss / n_total\n",
    "    epoch_acc = correct / n_total\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Eval Epoch: {epoch} | \"\n",
    "            f\"acc: {epoch_acc:.4f} | \"\n",
    "            f\"loss: {epoch_loss:.4f} | \"\n",
    "            f\"cls_loss: {epoch_cls_loss:.4f} | \"\n",
    "            f\"cnt_loss: {epoch_cnt_loss:.4f}\"\n",
    "        )\n",
    "    return epoch_loss, epoch_cls_loss, epoch_cnt_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baf72c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(root=\".\", batch_size=64, test_batch_size=1000, device=\"cpu\"):\n",
    "    num_workers = min(8, os.cpu_count() or 2)\n",
    "    pin = (device is not None and device.type == \"cuda\")\n",
    "    loader_kwargs = dict(\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    if num_workers > 0:\n",
    "        loader_kwargs[\"prefetch_factor\"] = 4\n",
    "\n",
    "    train_aug = Augmentation()\n",
    "    train_full = GSN(root=root, transform_relabel=train_aug)\n",
    "    test_full = GSN(root=root)\n",
    "\n",
    "    train_dataset = Subset(train_full, range(0, 9000))\n",
    "    test_dataset = Subset(test_full, range(9000, 10000))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **loader_kwargs)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, **loader_kwargs)\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9897ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 1e-3\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "\n",
    "cls_hidden = 256\n",
    "dropout = 0.3\n",
    "patience = 10\n",
    "# lambda_cnt = 1.0\n",
    "\n",
    "def train_model(\n",
    "    mode: str,\n",
    "    lambda_cnt: float,\n",
    "    log_interval: int = 10,\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    train_loader, test_loader = create_loaders(\n",
    "        root=\".\",\n",
    "        batch_size=64,\n",
    "        test_batch_size=1000,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    net = NeuralNetwork(cls_hidden, dropout).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_cls_loss\": [],\n",
    "        \"train_cnt_loss\": [],\n",
    "        \"eval_loss\": [],\n",
    "        \"eval_cls_loss\": [],\n",
    "        \"eval_cnt_loss\": [],\n",
    "        \"eval_acc\": [],\n",
    "    }\n",
    "\n",
    "    best_eval_loss = float(\"inf\")\n",
    "    best_acc = 0.0\n",
    "    best_state = None\n",
    "    bad_epochs = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss, train_cls_loss, train_cnt_loss = train_epoch(\n",
    "            net,\n",
    "            device,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            log_interval,\n",
    "            lambda_cnt,\n",
    "            mode,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        eval_loss, eval_cls_loss, eval_cnt_loss, eval_acc = eval_epoch(\n",
    "            net,\n",
    "            device,\n",
    "            test_loader,\n",
    "            epoch,\n",
    "            lambda_cnt,\n",
    "            mode,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_cls_loss\"].append(train_cls_loss)\n",
    "        history[\"train_cnt_loss\"].append(train_cnt_loss)\n",
    "        history[\"eval_loss\"].append(eval_loss)\n",
    "        history[\"eval_cls_loss\"].append(eval_cls_loss)\n",
    "        history[\"eval_cnt_loss\"].append(eval_cnt_loss)\n",
    "        history[\"eval_acc\"].append(eval_acc)\n",
    "\n",
    "        if eval_loss < best_eval_loss:\n",
    "            best_eval_loss = eval_loss\n",
    "            best_acc = eval_acc\n",
    "            best_state = {k: v.cpu().clone() for k,v in net.state_dict().items()}\n",
    "            bad_epochs = 0\n",
    "            best_epoch = epoch\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= patience:\n",
    "                print(\n",
    "                    f\"Early stop at epoch {epoch}. \"\n",
    "                    f\"Best val loss: {best_eval_loss:.4f}, \"\n",
    "                    f\"best acc: {best_acc:.4f}, \"\n",
    "                    f\"best epoch: {best_epoch}\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        net.load_state_dict(best_state)\n",
    "\n",
    "    return net, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd01ef39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Epoch: 1 | acc: 0.0120 | loss: 5.6244 | cls_loss: 4.3598 | cnt_loss: 1.2646\n",
      "Eval Epoch: 2 | acc: 0.1510 | loss: 3.7003 | cls_loss: 2.9016 | cnt_loss: 0.7987\n",
      "Eval Epoch: 3 | acc: 0.2370 | loss: 2.6300 | cls_loss: 2.1747 | cnt_loss: 0.4553\n",
      "Eval Epoch: 4 | acc: 0.2930 | loss: 2.2671 | cls_loss: 1.8795 | cnt_loss: 0.3876\n",
      "Eval Epoch: 5 | acc: 0.3230 | loss: 2.1656 | cls_loss: 1.7928 | cnt_loss: 0.3728\n",
      "Eval Epoch: 6 | acc: 0.3560 | loss: 1.9604 | cls_loss: 1.6431 | cnt_loss: 0.3173\n",
      "Eval Epoch: 7 | acc: 0.3300 | loss: 2.0548 | cls_loss: 1.7153 | cnt_loss: 0.3395\n",
      "Eval Epoch: 8 | acc: 0.3650 | loss: 1.8294 | cls_loss: 1.5480 | cnt_loss: 0.2815\n",
      "Eval Epoch: 9 | acc: 0.3770 | loss: 1.8168 | cls_loss: 1.5408 | cnt_loss: 0.2760\n",
      "Eval Epoch: 10 | acc: 0.4020 | loss: 1.6715 | cls_loss: 1.4181 | cnt_loss: 0.2534\n",
      "Eval Epoch: 11 | acc: 0.4150 | loss: 1.7058 | cls_loss: 1.4429 | cnt_loss: 0.2630\n",
      "Eval Epoch: 12 | acc: 0.4020 | loss: 1.6799 | cls_loss: 1.4325 | cnt_loss: 0.2473\n",
      "Eval Epoch: 13 | acc: 0.4210 | loss: 1.6419 | cls_loss: 1.3917 | cnt_loss: 0.2503\n",
      "Eval Epoch: 14 | acc: 0.4380 | loss: 1.5528 | cls_loss: 1.3247 | cnt_loss: 0.2281\n",
      "Eval Epoch: 15 | acc: 0.4270 | loss: 1.5620 | cls_loss: 1.3240 | cnt_loss: 0.2380\n",
      "Eval Epoch: 16 | acc: 0.4380 | loss: 1.4970 | cls_loss: 1.2798 | cnt_loss: 0.2172\n",
      "Eval Epoch: 17 | acc: 0.4220 | loss: 1.5953 | cls_loss: 1.3505 | cnt_loss: 0.2448\n",
      "Eval Epoch: 18 | acc: 0.4430 | loss: 1.5311 | cls_loss: 1.3105 | cnt_loss: 0.2206\n",
      "Eval Epoch: 19 | acc: 0.4150 | loss: 1.7174 | cls_loss: 1.4302 | cnt_loss: 0.2872\n",
      "Eval Epoch: 20 | acc: 0.4350 | loss: 1.5230 | cls_loss: 1.2993 | cnt_loss: 0.2237\n",
      "Eval Epoch: 21 | acc: 0.4550 | loss: 1.5215 | cls_loss: 1.2989 | cnt_loss: 0.2226\n",
      "Eval Epoch: 22 | acc: 0.4810 | loss: 1.4870 | cls_loss: 1.2648 | cnt_loss: 0.2222\n",
      "Eval Epoch: 23 | acc: 0.4780 | loss: 1.4049 | cls_loss: 1.2046 | cnt_loss: 0.2003\n",
      "Eval Epoch: 24 | acc: 0.4870 | loss: 1.4037 | cls_loss: 1.2123 | cnt_loss: 0.1914\n",
      "Eval Epoch: 25 | acc: 0.4840 | loss: 1.4365 | cls_loss: 1.2383 | cnt_loss: 0.1982\n",
      "Eval Epoch: 26 | acc: 0.4790 | loss: 1.3973 | cls_loss: 1.1978 | cnt_loss: 0.1995\n",
      "Eval Epoch: 27 | acc: 0.5020 | loss: 1.3766 | cls_loss: 1.1841 | cnt_loss: 0.1925\n",
      "Eval Epoch: 28 | acc: 0.4670 | loss: 1.4552 | cls_loss: 1.2441 | cnt_loss: 0.2110\n",
      "Eval Epoch: 29 | acc: 0.5080 | loss: 1.3807 | cls_loss: 1.1882 | cnt_loss: 0.1925\n",
      "Eval Epoch: 30 | acc: 0.5060 | loss: 1.3837 | cls_loss: 1.1899 | cnt_loss: 0.1938\n",
      "Eval Epoch: 31 | acc: 0.5230 | loss: 1.3446 | cls_loss: 1.1577 | cnt_loss: 0.1869\n",
      "Eval Epoch: 32 | acc: 0.5070 | loss: 1.3947 | cls_loss: 1.2050 | cnt_loss: 0.1897\n",
      "Eval Epoch: 33 | acc: 0.5210 | loss: 1.3748 | cls_loss: 1.1817 | cnt_loss: 0.1931\n",
      "Eval Epoch: 34 | acc: 0.5350 | loss: 1.3273 | cls_loss: 1.1379 | cnt_loss: 0.1894\n",
      "Eval Epoch: 35 | acc: 0.4820 | loss: 1.4927 | cls_loss: 1.2761 | cnt_loss: 0.2166\n",
      "Eval Epoch: 36 | acc: 0.5000 | loss: 1.3865 | cls_loss: 1.1934 | cnt_loss: 0.1931\n",
      "Eval Epoch: 37 | acc: 0.5200 | loss: 1.3595 | cls_loss: 1.1790 | cnt_loss: 0.1806\n",
      "Eval Epoch: 38 | acc: 0.5100 | loss: 1.3526 | cls_loss: 1.1733 | cnt_loss: 0.1793\n",
      "Eval Epoch: 39 | acc: 0.5170 | loss: 1.3479 | cls_loss: 1.1702 | cnt_loss: 0.1778\n",
      "Eval Epoch: 40 | acc: 0.5060 | loss: 1.4040 | cls_loss: 1.2222 | cnt_loss: 0.1819\n",
      "Eval Epoch: 41 | acc: 0.5380 | loss: 1.3613 | cls_loss: 1.1781 | cnt_loss: 0.1832\n",
      "Eval Epoch: 42 | acc: 0.5210 | loss: 1.3488 | cls_loss: 1.1716 | cnt_loss: 0.1772\n",
      "Eval Epoch: 43 | acc: 0.5310 | loss: 1.4014 | cls_loss: 1.2223 | cnt_loss: 0.1791\n",
      "Eval Epoch: 44 | acc: 0.4690 | loss: 1.5973 | cls_loss: 1.3899 | cnt_loss: 0.2074\n",
      "Early stop at epoch 44. Best val loss: 1.3273, best acc: 0.5350, best epoch: 34\n"
     ]
    }
   ],
   "source": [
    "net, history = train_model(\n",
    "    mode = \"multitask\",\n",
    "    lambda_cnt=1.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0241b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"0535-net.pt\"\n",
    "torch.save(net.state_dict(), save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
