{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077df0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
    "# !unzip data_gsn.zip &> /dev/null\n",
    "# !rm data_gsn.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f3cd8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f837177e6f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02cf7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSN(Dataset):\n",
    "    def __init__(self, root, transform=None, transform_relabel=None):\n",
    "        self.data_dir = os.path.join(root, \"data\")\n",
    "        self.transform = transform\n",
    "        self.transform_relabel = transform_relabel\n",
    "\n",
    "        df = pd.read_csv(os.path.join(self.data_dir, \"labels.csv\"))\n",
    "        self.names = df[\"name\"].tolist()\n",
    "        cols = [\"squares\", \"circles\", \"up\", \"right\", \"down\", \"left\"]\n",
    "        self.labels = torch.tensor(df[cols].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        name = self.names[index]\n",
    "        img_path = os.path.join(self.data_dir, name)\n",
    "\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        img = transforms.ToTensor()(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        cnt = self.labels[index]\n",
    "\n",
    "        if self.transform_relabel:\n",
    "            img, cnt = self.transform_relabel(img, cnt)\n",
    "        \n",
    "        cls = self.counts_to_class_id(cnt)\n",
    "\n",
    "        return img, cls, cnt\n",
    "    \n",
    "    def counts_to_class_id(self, counts):\n",
    "        PAIRS = [(i, j) for i in range(6) for j in range(i + 1, 6)]\n",
    "        if isinstance(counts, torch.Tensor):\n",
    "            c = counts.detach().cpu().tolist()\n",
    "        else:\n",
    "            c = list(counts)\n",
    "\n",
    "        nz = [i for i, v in enumerate(c) if v > 0]\n",
    "        if len(nz) != 2:\n",
    "            raise ValueError(f\"Expected exactly 2 nonzero counts, got {len(nz)}: {c}\")\n",
    "        if sum(c) != 10:\n",
    "            raise ValueError(f\"Expected counts to sum to 10, got {sum(c)}: {c}\")\n",
    "\n",
    "        a, b = sorted(nz)\n",
    "        ca = int(c[a])\n",
    "        pair_index = PAIRS.index((a, b))\n",
    "\n",
    "        class_id = pair_index * 9 + (ca - 1)\n",
    "        return class_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5db8493",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augumentation:\n",
    "    def __init__(self, p_hflip=0.5, p_vflip=0.5):\n",
    "        self.p_hflip = p_hflip\n",
    "        self.p_vflip = p_vflip\n",
    "\n",
    "    def __call__(self, img, cnt):\n",
    "        cnt = cnt.clone()\n",
    "\n",
    "        k = torch.randint(0, 4, (1,)).item()\n",
    "        if k > 0:\n",
    "            img = torch.rot90(img, k=-k, dims=[1,2])\n",
    "            dirs = cnt[2:6]\n",
    "            dirs = torch.roll(dirs, shifts=k)\n",
    "            cnt[2:6] = dirs\n",
    "\n",
    "        if torch.rand(1).item() < self.p_hflip:\n",
    "            img = torch.flip(img, dims=[2])\n",
    "            cnt[[3, 5]] = cnt[[5, 3]]\n",
    "        \n",
    "        if torch.rand(1).item() < self.p_vflip:\n",
    "            img = torch.flip(img, dims=[1])\n",
    "            cnt[[2, 4]] = cnt[[4, 2]]\n",
    "\n",
    "        return img, cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6103b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, cls_hidden=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = nn.Sequential(                              # (64, 1, 28, 28)\n",
    "            nn.Conv2d(1, 8, 3, stride=1, padding=1), nn.ReLU(),     # (64, 8, 28 28)  \n",
    "            nn.Conv2d(8, 16, 3, stride=1, padding=1), nn.ReLU(),    # (64, 16, 28, 28)\n",
    "            nn.Conv2d(16, 32, 3, stride=1, padding=1), nn.ReLU(),   # (64, 32, 28, 28)\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1), nn.ReLU(),   # (64, 64, 28, 28)\n",
    "            nn.Flatten(start_dim=1),                                # (64, 64 * 28 * 28)\n",
    "            nn.Linear(64 * 28 * 28, 256), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.head_cls = nn.Sequential(\n",
    "            nn.Linear(256, cls_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            # nn.Linear(cls_hidden, cls_hidden),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(p=dropout),\n",
    "            nn.Linear(cls_hidden, 135),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.head_cnt = nn.Sequential(\n",
    "            nn.Linear(256, 6)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        cls = self.head_cls(x)  # (64, 135)\n",
    "        cnt = self.head_cnt(x)  # (64, 6)\n",
    "\n",
    "        return cls, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc4775f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    net: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    log_interval: int,\n",
    "    lambda_cnt: float,\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    net.train()\n",
    "    total_loss = 0.0\n",
    "    total_cls_loss = 0.0\n",
    "    total_cnt_loss = 0.0\n",
    "    n_total = 0\n",
    "    \n",
    "    for batch_idx, (img, cls_target, cnt_target) in enumerate(train_loader):\n",
    "        img, cls_target, cnt_target = img.to(device), cls_target.long().to(device), cnt_target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cls_pred, cnt_pred = net(img)\n",
    "        \n",
    "        cls_loss = F.nll_loss(cls_pred, cls_target)\n",
    "        cnt_loss = F.smooth_l1_loss(cnt_pred, cnt_target)\n",
    "        loss = cls_loss + lambda_cnt * cnt_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        B = len(img)\n",
    "\n",
    "        total_loss += loss.item() * B\n",
    "        total_cls_loss += cls_loss.item() * B\n",
    "        total_cnt_loss += cnt_loss.item() * B\n",
    "        n_total += B\n",
    "\n",
    "        if verbose and batch_idx % log_interval == 0:\n",
    "            done = batch_idx * B\n",
    "            total = len(train_loader.dataset)\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch} [{done}/{total} images ({done / total:.0%})]\\t\"\n",
    "                + f\"Loss: {loss.item():.6f}\"\n",
    "            )\n",
    "\n",
    "    epoch_loss = total_loss / n_total\n",
    "    epoch_cls_loss = total_cls_loss / n_total\n",
    "    epoch_cnt_loss = total_cnt_loss / n_total\n",
    "    return epoch_loss, epoch_cls_loss, epoch_cnt_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "437bc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(\n",
    "    net: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    epoch: int,\n",
    "    lambda_cnt: float,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    net.eval()\n",
    "    total_loss = 0.0\n",
    "    total_cls_loss = 0.0\n",
    "    total_cnt_loss = 0.0\n",
    "    n_total = 0\n",
    "    correct = 0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, cls_target, cnt_target in test_loader:\n",
    "            img, cls_target, cnt_target = img.to(device), cls_target.long().to(device), cnt_target.to(device)\n",
    "\n",
    "            cls_pred, cnt_pred = net(img)\n",
    "            cls_loss = F.nll_loss(cls_pred, cls_target)\n",
    "            cnt_loss = F.smooth_l1_loss(cnt_pred, cnt_target)\n",
    "            loss = cls_loss + lambda_cnt * cnt_loss\n",
    "\n",
    "            total_loss += loss.item() * len(img)\n",
    "            total_cls_loss += cls_loss.item() * len(img)\n",
    "            total_cnt_loss += cnt_loss.item() * len(img)\n",
    "            n_total += len(img)\n",
    "\n",
    "            pred = cls_pred.argmax(dim=1)\n",
    "            correct += (pred == cls_target).sum().item()\n",
    "\n",
    "    epoch_loss = total_loss / n_total\n",
    "    epoch_cls_loss = total_cls_loss / n_total\n",
    "    epoch_cnt_loss = total_cnt_loss / n_total\n",
    "    epoch_acc = correct / n_total\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Eval Epoch: {epoch} accuracy: {epoch_acc:.4f} epoch_loss: {epoch_loss:.4f} epoch_cls_loss: {epoch_cls_loss:.4f} epoch_cnt_loss: {epoch_cnt_loss:.4f}\\n\")\n",
    "    return epoch_loss, epoch_cls_loss, epoch_cnt_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da83da5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Epoch: 1 accuracy: 0.0260 epoch_loss: 5.4615 epoch_cls_loss: 4.2501 epoch_cnt_loss: 1.2114\n",
      "\n",
      "Eval Epoch: 2 accuracy: 0.1900 epoch_loss: 3.1856 epoch_cls_loss: 2.5819 epoch_cnt_loss: 0.6038\n",
      "\n",
      "Eval Epoch: 3 accuracy: 0.2420 epoch_loss: 2.5431 epoch_cls_loss: 2.0906 epoch_cnt_loss: 0.4525\n",
      "\n",
      "Eval Epoch: 4 accuracy: 0.2860 epoch_loss: 2.2471 epoch_cls_loss: 1.8727 epoch_cnt_loss: 0.3743\n",
      "\n",
      "Eval Epoch: 5 accuracy: 0.3160 epoch_loss: 1.9976 epoch_cls_loss: 1.6695 epoch_cnt_loss: 0.3281\n",
      "\n",
      "Eval Epoch: 6 accuracy: 0.3340 epoch_loss: 2.0079 epoch_cls_loss: 1.6863 epoch_cnt_loss: 0.3216\n",
      "\n",
      "Eval Epoch: 7 accuracy: 0.3550 epoch_loss: 1.8379 epoch_cls_loss: 1.5485 epoch_cnt_loss: 0.2894\n",
      "\n",
      "Eval Epoch: 8 accuracy: 0.3490 epoch_loss: 1.9242 epoch_cls_loss: 1.5970 epoch_cnt_loss: 0.3272\n",
      "\n",
      "Eval Epoch: 9 accuracy: 0.3860 epoch_loss: 1.7287 epoch_cls_loss: 1.4614 epoch_cnt_loss: 0.2672\n",
      "\n",
      "Eval Epoch: 10 accuracy: 0.3620 epoch_loss: 1.8894 epoch_cls_loss: 1.5862 epoch_cnt_loss: 0.3032\n",
      "\n",
      "Eval Epoch: 11 accuracy: 0.4010 epoch_loss: 1.6775 epoch_cls_loss: 1.4224 epoch_cnt_loss: 0.2552\n",
      "\n",
      "Eval Epoch: 12 accuracy: 0.3920 epoch_loss: 1.6957 epoch_cls_loss: 1.4398 epoch_cnt_loss: 0.2558\n",
      "\n",
      "Eval Epoch: 13 accuracy: 0.4280 epoch_loss: 1.5924 epoch_cls_loss: 1.3646 epoch_cnt_loss: 0.2278\n",
      "\n",
      "Eval Epoch: 14 accuracy: 0.4100 epoch_loss: 1.6391 epoch_cls_loss: 1.3943 epoch_cnt_loss: 0.2448\n",
      "\n",
      "Eval Epoch: 15 accuracy: 0.4230 epoch_loss: 1.5611 epoch_cls_loss: 1.3424 epoch_cnt_loss: 0.2187\n",
      "\n",
      "Eval Epoch: 16 accuracy: 0.4330 epoch_loss: 1.5441 epoch_cls_loss: 1.3314 epoch_cnt_loss: 0.2127\n",
      "\n",
      "Eval Epoch: 17 accuracy: 0.4430 epoch_loss: 1.4984 epoch_cls_loss: 1.2798 epoch_cnt_loss: 0.2185\n",
      "\n",
      "Eval Epoch: 18 accuracy: 0.4530 epoch_loss: 1.4855 epoch_cls_loss: 1.2730 epoch_cnt_loss: 0.2124\n",
      "\n",
      "Eval Epoch: 19 accuracy: 0.4560 epoch_loss: 1.4579 epoch_cls_loss: 1.2550 epoch_cnt_loss: 0.2030\n",
      "\n",
      "Eval Epoch: 20 accuracy: 0.4440 epoch_loss: 1.5262 epoch_cls_loss: 1.3166 epoch_cnt_loss: 0.2096\n",
      "\n",
      "Eval Epoch: 21 accuracy: 0.4780 epoch_loss: 1.4244 epoch_cls_loss: 1.2199 epoch_cnt_loss: 0.2045\n",
      "\n",
      "Eval Epoch: 22 accuracy: 0.4730 epoch_loss: 1.4366 epoch_cls_loss: 1.2326 epoch_cnt_loss: 0.2039\n",
      "\n",
      "Eval Epoch: 23 accuracy: 0.4710 epoch_loss: 1.4890 epoch_cls_loss: 1.2815 epoch_cnt_loss: 0.2075\n",
      "\n",
      "Eval Epoch: 24 accuracy: 0.4790 epoch_loss: 1.4694 epoch_cls_loss: 1.2601 epoch_cnt_loss: 0.2092\n",
      "\n",
      "Eval Epoch: 25 accuracy: 0.4380 epoch_loss: 1.4878 epoch_cls_loss: 1.2875 epoch_cnt_loss: 0.2002\n",
      "\n",
      "Eval Epoch: 26 accuracy: 0.4750 epoch_loss: 1.3999 epoch_cls_loss: 1.2093 epoch_cnt_loss: 0.1906\n",
      "\n",
      "Eval Epoch: 27 accuracy: 0.4730 epoch_loss: 1.4319 epoch_cls_loss: 1.2354 epoch_cnt_loss: 0.1965\n",
      "\n",
      "Eval Epoch: 28 accuracy: 0.4620 epoch_loss: 1.4915 epoch_cls_loss: 1.2985 epoch_cnt_loss: 0.1930\n",
      "\n",
      "Eval Epoch: 29 accuracy: 0.4760 epoch_loss: 1.5126 epoch_cls_loss: 1.3072 epoch_cnt_loss: 0.2053\n",
      "\n",
      "Eval Epoch: 30 accuracy: 0.5030 epoch_loss: 1.4110 epoch_cls_loss: 1.2263 epoch_cnt_loss: 0.1847\n",
      "\n",
      "Eval Epoch: 31 accuracy: 0.5120 epoch_loss: 1.3731 epoch_cls_loss: 1.1927 epoch_cnt_loss: 0.1804\n",
      "\n",
      "Eval Epoch: 32 accuracy: 0.4890 epoch_loss: 1.4547 epoch_cls_loss: 1.2689 epoch_cnt_loss: 0.1859\n",
      "\n",
      "Eval Epoch: 33 accuracy: 0.4910 epoch_loss: 1.4661 epoch_cls_loss: 1.2788 epoch_cnt_loss: 0.1873\n",
      "\n",
      "Eval Epoch: 34 accuracy: 0.4840 epoch_loss: 1.4819 epoch_cls_loss: 1.2970 epoch_cnt_loss: 0.1849\n",
      "\n",
      "Eval Epoch: 35 accuracy: 0.4560 epoch_loss: 1.5752 epoch_cls_loss: 1.3657 epoch_cnt_loss: 0.2095\n",
      "\n",
      "Eval Epoch: 36 accuracy: 0.5050 epoch_loss: 1.4647 epoch_cls_loss: 1.2820 epoch_cnt_loss: 0.1827\n",
      "\n",
      "Eval Epoch: 37 accuracy: 0.4650 epoch_loss: 1.6090 epoch_cls_loss: 1.4170 epoch_cnt_loss: 0.1920\n",
      "\n",
      "Eval Epoch: 38 accuracy: 0.4920 epoch_loss: 1.5386 epoch_cls_loss: 1.3526 epoch_cnt_loss: 0.1860\n",
      "\n",
      "Eval Epoch: 39 accuracy: 0.4850 epoch_loss: 1.4809 epoch_cls_loss: 1.2976 epoch_cnt_loss: 0.1833\n",
      "\n",
      "Eval Epoch: 40 accuracy: 0.5170 epoch_loss: 1.4866 epoch_cls_loss: 1.2970 epoch_cnt_loss: 0.1895\n",
      "\n",
      "Eval Epoch: 41 accuracy: 0.5060 epoch_loss: 1.4864 epoch_cls_loss: 1.3050 epoch_cnt_loss: 0.1814\n",
      "\n",
      "Early stop at epoch 41. Best eval loss: 1.3731, Best accuracy = 0.512 Optimal epochs: 31\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 1e-3\n",
    "log_interval = 10\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "lambda_cnt = 1.0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "num_workers = min(8, os.cpu_count() or 2)\n",
    "pin = (device.type == \"cuda\")\n",
    "train_kwargs = dict(\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "if num_workers > 0:\n",
    "    train_kwargs[\"prefetch_factor\"] = 4\n",
    "\n",
    "train_augumentation = Augumentation()\n",
    "\n",
    "train_dataset = GSN(root=\".\", transform_relabel=train_augumentation)\n",
    "test_dataset = GSN(root=\".\")\n",
    "\n",
    "train_dataset = Subset(train_dataset, range(0, 9000))\n",
    "test_dataset = Subset(test_dataset, range(9000, 10000))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **train_kwargs)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, **train_kwargs)\n",
    "\n",
    "net = NeuralNetwork(cls_hidden=256, dropout=0.3).to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "best_eval_loss = float(\"inf\")\n",
    "best_accuracy = 0.0\n",
    "best_state = None\n",
    "bad_epochs = 0\n",
    "bad_epochs_patience = 10\n",
    "optimal_epochs = epochs\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_cls_loss, train_cnt_loss = train_epoch(\n",
    "        net,\n",
    "        device,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        epoch,\n",
    "        log_interval,\n",
    "        lambda_cnt,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    eval_loss, eval_cls_loss, eval_cnt_loss, eval_acc = eval_epoch(\n",
    "        net,\n",
    "        device,\n",
    "        test_loader,\n",
    "        epoch,\n",
    "        lambda_cnt,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    if eval_loss < best_eval_loss:\n",
    "        best_eval_loss = eval_loss\n",
    "        best_accuracy = eval_acc\n",
    "        best_state = {k: v.cpu().clone() for k,v in net.state_dict().items()}\n",
    "        bad_epochs = 0\n",
    "        optimal_epochs = epoch\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "        if bad_epochs >= bad_epochs_patience:\n",
    "            print(f\"Early stop at epoch {epoch}. Best eval loss: {best_eval_loss:.4f}, Best accuracy = {best_accuracy} Optimal epochs: {optimal_epochs}\")\n",
    "            break\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    eval_losses.append(eval_loss)\n",
    "\n",
    "if best_state is not None:\n",
    "    net.load_state_dict(best_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0241b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"0512-net.pt\"\n",
    "torch.save(net.state_dict(), save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
