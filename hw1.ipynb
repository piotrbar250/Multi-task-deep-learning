{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077df0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
    "# !unzip data_gsn.zip &> /dev/null\n",
    "# !rm data_gsn.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f3cd8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f657013e490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02cf7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSN(Dataset):\n",
    "    def __init__(self, root, transform=None, transform_relabel=None):\n",
    "        self.data_dir = os.path.join(root, \"data\")\n",
    "        self.transform = transform\n",
    "        self.transform_relabel = transform_relabel\n",
    "\n",
    "        df = pd.read_csv(os.path.join(self.data_dir, \"labels.csv\"))\n",
    "        self.names = df[\"name\"].tolist()\n",
    "        cols = [\"squares\", \"circles\", \"up\", \"right\", \"down\", \"left\"]\n",
    "        self.labels = torch.tensor(df[cols].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        name = self.names[index]\n",
    "        img_path = os.path.join(self.data_dir, name)\n",
    "\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        img = transforms.ToTensor()(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        cnt = self.labels[index]\n",
    "\n",
    "        if self.transform_relabel:\n",
    "            img, cnt = self.transform_relabel(img, cnt)\n",
    "        \n",
    "        cls = self.counts_to_class_id(cnt)\n",
    "\n",
    "        return img, cls, cnt\n",
    "    \n",
    "    def counts_to_class_id(self, counts):\n",
    "        PAIRS = [(i, j) for i in range(6) for j in range(i + 1, 6)]\n",
    "        if isinstance(counts, torch.Tensor):\n",
    "            c = counts.detach().cpu().tolist()\n",
    "        else:\n",
    "            c = list(counts)\n",
    "\n",
    "        nz = [i for i, v in enumerate(c) if v > 0]\n",
    "        if len(nz) != 2:\n",
    "            raise ValueError(f\"Expected exactly 2 nonzero counts, got {len(nz)}: {c}\")\n",
    "        if sum(c) != 10:\n",
    "            raise ValueError(f\"Expected counts to sum to 10, got {sum(c)}: {c}\")\n",
    "\n",
    "        a, b = sorted(nz)\n",
    "        ca = int(c[a])\n",
    "        pair_index = PAIRS.index((a, b))\n",
    "\n",
    "        class_id = pair_index * 9 + (ca - 1)\n",
    "        return class_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5db8493",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augumentation:\n",
    "    def __init__(self, p_hflip=0.5, p_vflip=0.5):\n",
    "        self.p_hflip = p_hflip\n",
    "        self.p_vflip = p_vflip\n",
    "\n",
    "    def __call__(self, img, cnt):\n",
    "        cnt = cnt.clone()\n",
    "\n",
    "        k = torch.randint(0, 4, (1,)).item()\n",
    "        if k > 0:\n",
    "            img = torch.rot90(img, k=-k, dims=[1,2])\n",
    "            dirs = cnt[2:6]\n",
    "            dirs = torch.roll(dirs, shifts=k)\n",
    "            cnt[2:6] = dirs\n",
    "\n",
    "        if torch.rand(1).item() < self.p_hflip:\n",
    "            img = torch.flip(img, dims=[2])\n",
    "            cnt[[3, 5]] = cnt[[5, 3]]\n",
    "        \n",
    "        if torch.rand(1).item() < self.p_vflip:\n",
    "            img = torch.flip(img, dims=[1])\n",
    "            cnt[[2, 4]] = cnt[[4, 2]]\n",
    "\n",
    "        return img, cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6103b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, cls_hidden=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = nn.Sequential(                              # (64, 1, 28, 28)\n",
    "            nn.Conv2d(1, 8, 3, stride=1, padding=1), nn.ReLU(),     # (64, 8, 28 28)  \n",
    "            nn.Conv2d(8, 16, 3, stride=1, padding=1), nn.ReLU(),    # (64, 16, 28, 28)\n",
    "            nn.Conv2d(16, 32, 3, stride=1, padding=1), nn.ReLU(),   # (64, 32, 28, 28)\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1), nn.ReLU(),   # (64, 64, 28, 28)\n",
    "            nn.Flatten(start_dim=1),                                # (64, 64 * 28 * 28)\n",
    "            nn.Linear(64 * 28 * 28, 256), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.head_cls = nn.Sequential(\n",
    "            nn.Linear(256, cls_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            # nn.Linear(cls_hidden, cls_hidden),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(p=dropout),\n",
    "            nn.Linear(cls_hidden, 135),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.head_cnt = nn.Sequential(\n",
    "            nn.Linear(256, 6)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        cls = self.head_cls(x)  # (64, 135)\n",
    "        cnt = self.head_cnt(x)  # (64, 6)\n",
    "\n",
    "        return cls, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc4775f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    net: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    log_interval: int,\n",
    "    lambda_cnt: float,\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    net.train()\n",
    "    total_loss = 0.0\n",
    "    total_cls_loss = 0.0\n",
    "    total_cnt_loss = 0.0\n",
    "    n_total = 0\n",
    "    \n",
    "    for batch_idx, (img, cls_target, cnt_target) in enumerate(train_loader):\n",
    "        img, cls_target, cnt_target = img.to(device), cls_target.long().to(device), cnt_target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cls_pred, cnt_pred = net(img)\n",
    "        \n",
    "        cls_loss = F.nll_loss(cls_pred, cls_target)\n",
    "        cnt_loss = F.smooth_l1_loss(cnt_pred, cnt_target)\n",
    "        loss = cls_loss + lambda_cnt * cnt_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        B = len(img)\n",
    "\n",
    "        total_loss += loss.item() * B\n",
    "        total_cls_loss += cls_loss.item() * B\n",
    "        total_cnt_loss += cnt_loss.item() * B\n",
    "        n_total += B\n",
    "\n",
    "        if verbose and batch_idx % log_interval == 0:\n",
    "            done = batch_idx * B\n",
    "            total = len(train_loader.dataset)\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch} [{done}/{total} images ({done / total:.0%})]\\t\"\n",
    "                + f\"Loss: {loss.item():.6f}\"\n",
    "            )\n",
    "\n",
    "    epoch_loss = total_loss / n_total\n",
    "    epoch_cls_loss = total_cls_loss / n_total\n",
    "    epoch_cnt_loss = total_cnt_loss / n_total\n",
    "    return epoch_loss, epoch_cls_loss, epoch_cnt_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "437bc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(\n",
    "    net: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    epoch: int,\n",
    "    lambda_cnt: float,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    net.eval()\n",
    "    total_loss = 0.0\n",
    "    total_cls_loss = 0.0\n",
    "    total_cnt_loss = 0.0\n",
    "    n_total = 0\n",
    "    correct = 0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, cls_target, cnt_target in test_loader:\n",
    "            img, cls_target, cnt_target = img.to(device), cls_target.long().to(device), cnt_target.to(device)\n",
    "\n",
    "            cls_pred, cnt_pred = net(img)\n",
    "            cls_loss = F.nll_loss(cls_pred, cls_target)\n",
    "            cnt_loss = F.smooth_l1_loss(cnt_pred, cnt_target)\n",
    "            loss = cls_loss + lambda_cnt * cnt_loss\n",
    "\n",
    "            total_loss += loss.item() * len(img)\n",
    "            total_cls_loss += cls_loss.item() * len(img)\n",
    "            total_cnt_loss += cnt_loss.item() * len(img)\n",
    "            n_total += len(img)\n",
    "\n",
    "            pred = cls_pred.argmax(dim=1)\n",
    "            correct += (pred == cls_target).sum().item()\n",
    "\n",
    "    epoch_loss = total_loss / n_total\n",
    "    epoch_cls_loss = total_cls_loss / n_total\n",
    "    epoch_cnt_loss = total_cnt_loss / n_total\n",
    "    epoch_acc = correct / n_total\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Eval Epoch: {epoch} accuracy: {epoch_acc:.4f} epoch_loss: {epoch_loss:.4f} epoch_cls_loss: {epoch_cls_loss:.4f} epoch_cnt_loss: {epoch_cnt_loss:.4f}\\n\")\n",
    "    return epoch_loss, epoch_cls_loss, epoch_cnt_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83da5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Epoch: 1 accuracy: 0.0210 epoch_loss: 5.7944 epoch_cls_loss: 4.4794 epoch_cnt_loss: 1.3150\n",
      "\n",
      "Eval Epoch: 2 accuracy: 0.1330 epoch_loss: 3.7622 epoch_cls_loss: 3.0201 epoch_cnt_loss: 0.7420\n",
      "\n",
      "Eval Epoch: 3 accuracy: 0.2200 epoch_loss: 2.8700 epoch_cls_loss: 2.2961 epoch_cnt_loss: 0.5739\n",
      "\n",
      "Eval Epoch: 4 accuracy: 0.2710 epoch_loss: 2.4528 epoch_cls_loss: 2.0054 epoch_cnt_loss: 0.4474\n",
      "\n",
      "Eval Epoch: 5 accuracy: 0.3040 epoch_loss: 2.2843 epoch_cls_loss: 1.8837 epoch_cnt_loss: 0.4006\n",
      "\n",
      "Eval Epoch: 6 accuracy: 0.3380 epoch_loss: 2.0211 epoch_cls_loss: 1.6906 epoch_cnt_loss: 0.3304\n",
      "\n",
      "Eval Epoch: 7 accuracy: 0.3430 epoch_loss: 1.9482 epoch_cls_loss: 1.6378 epoch_cnt_loss: 0.3104\n",
      "\n",
      "Eval Epoch: 8 accuracy: 0.3260 epoch_loss: 1.9912 epoch_cls_loss: 1.6679 epoch_cnt_loss: 0.3233\n",
      "\n",
      "Eval Epoch: 9 accuracy: 0.3720 epoch_loss: 1.9217 epoch_cls_loss: 1.6166 epoch_cnt_loss: 0.3051\n",
      "\n",
      "Eval Epoch: 10 accuracy: 0.3800 epoch_loss: 1.8093 epoch_cls_loss: 1.5205 epoch_cnt_loss: 0.2888\n",
      "\n",
      "Eval Epoch: 11 accuracy: 0.3940 epoch_loss: 1.7510 epoch_cls_loss: 1.4764 epoch_cnt_loss: 0.2745\n",
      "\n",
      "Eval Epoch: 12 accuracy: 0.3950 epoch_loss: 1.7166 epoch_cls_loss: 1.4520 epoch_cnt_loss: 0.2646\n",
      "\n",
      "Eval Epoch: 13 accuracy: 0.4280 epoch_loss: 1.6413 epoch_cls_loss: 1.3992 epoch_cnt_loss: 0.2421\n",
      "\n",
      "Eval Epoch: 14 accuracy: 0.3950 epoch_loss: 1.7128 epoch_cls_loss: 1.4683 epoch_cnt_loss: 0.2445\n",
      "\n",
      "Eval Epoch: 15 accuracy: 0.4170 epoch_loss: 1.6623 epoch_cls_loss: 1.4134 epoch_cnt_loss: 0.2489\n",
      "\n",
      "Eval Epoch: 16 accuracy: 0.4480 epoch_loss: 1.5339 epoch_cls_loss: 1.3116 epoch_cnt_loss: 0.2223\n",
      "\n",
      "Eval Epoch: 17 accuracy: 0.4170 epoch_loss: 1.6274 epoch_cls_loss: 1.3689 epoch_cnt_loss: 0.2584\n",
      "\n",
      "Eval Epoch: 18 accuracy: 0.4480 epoch_loss: 1.5115 epoch_cls_loss: 1.2949 epoch_cnt_loss: 0.2166\n",
      "\n",
      "Eval Epoch: 19 accuracy: 0.4440 epoch_loss: 1.4910 epoch_cls_loss: 1.2769 epoch_cnt_loss: 0.2141\n",
      "\n",
      "Eval Epoch: 20 accuracy: 0.4330 epoch_loss: 1.5340 epoch_cls_loss: 1.3057 epoch_cnt_loss: 0.2284\n",
      "\n",
      "Eval Epoch: 21 accuracy: 0.4360 epoch_loss: 1.5349 epoch_cls_loss: 1.3127 epoch_cnt_loss: 0.2222\n",
      "\n",
      "Eval Epoch: 22 accuracy: 0.4480 epoch_loss: 1.5452 epoch_cls_loss: 1.3231 epoch_cnt_loss: 0.2221\n",
      "\n",
      "Eval Epoch: 23 accuracy: 0.4340 epoch_loss: 1.5157 epoch_cls_loss: 1.2902 epoch_cnt_loss: 0.2255\n",
      "\n",
      "Eval Epoch: 24 accuracy: 0.4680 epoch_loss: 1.4489 epoch_cls_loss: 1.2457 epoch_cnt_loss: 0.2033\n",
      "\n",
      "Eval Epoch: 25 accuracy: 0.4190 epoch_loss: 1.5405 epoch_cls_loss: 1.3302 epoch_cnt_loss: 0.2103\n",
      "\n",
      "Eval Epoch: 26 accuracy: 0.4740 epoch_loss: 1.4135 epoch_cls_loss: 1.2152 epoch_cnt_loss: 0.1983\n",
      "\n",
      "Eval Epoch: 27 accuracy: 0.4550 epoch_loss: 1.4804 epoch_cls_loss: 1.2696 epoch_cnt_loss: 0.2108\n",
      "\n",
      "Eval Epoch: 28 accuracy: 0.4680 epoch_loss: 1.4419 epoch_cls_loss: 1.2317 epoch_cnt_loss: 0.2101\n",
      "\n",
      "Eval Epoch: 29 accuracy: 0.4810 epoch_loss: 1.4149 epoch_cls_loss: 1.2229 epoch_cnt_loss: 0.1920\n",
      "\n",
      "Eval Epoch: 30 accuracy: 0.4830 epoch_loss: 1.3862 epoch_cls_loss: 1.1926 epoch_cnt_loss: 0.1936\n",
      "\n",
      "Eval Epoch: 31 accuracy: 0.4900 epoch_loss: 1.3986 epoch_cls_loss: 1.2065 epoch_cnt_loss: 0.1921\n",
      "\n",
      "Eval Epoch: 32 accuracy: 0.4710 epoch_loss: 1.4132 epoch_cls_loss: 1.2202 epoch_cnt_loss: 0.1930\n",
      "\n",
      "Eval Epoch: 33 accuracy: 0.4970 epoch_loss: 1.3465 epoch_cls_loss: 1.1589 epoch_cnt_loss: 0.1876\n",
      "\n",
      "Eval Epoch: 34 accuracy: 0.4540 epoch_loss: 1.5400 epoch_cls_loss: 1.3150 epoch_cnt_loss: 0.2250\n",
      "\n",
      "Eval Epoch: 35 accuracy: 0.5000 epoch_loss: 1.3681 epoch_cls_loss: 1.1716 epoch_cnt_loss: 0.1965\n",
      "\n",
      "Eval Epoch: 36 accuracy: 0.4800 epoch_loss: 1.4185 epoch_cls_loss: 1.2195 epoch_cnt_loss: 0.1990\n",
      "\n",
      "Eval Epoch: 37 accuracy: 0.4760 epoch_loss: 1.5136 epoch_cls_loss: 1.3114 epoch_cnt_loss: 0.2023\n",
      "\n",
      "Eval Epoch: 38 accuracy: 0.4730 epoch_loss: 1.3873 epoch_cls_loss: 1.1968 epoch_cnt_loss: 0.1904\n",
      "\n",
      "Eval Epoch: 39 accuracy: 0.4920 epoch_loss: 1.3585 epoch_cls_loss: 1.1676 epoch_cnt_loss: 0.1909\n",
      "\n",
      "Eval Epoch: 40 accuracy: 0.5030 epoch_loss: 1.3577 epoch_cls_loss: 1.1700 epoch_cnt_loss: 0.1877\n",
      "\n",
      "Eval Epoch: 41 accuracy: 0.4970 epoch_loss: 1.3797 epoch_cls_loss: 1.1873 epoch_cnt_loss: 0.1925\n",
      "\n",
      "Eval Epoch: 42 accuracy: 0.4690 epoch_loss: 1.4046 epoch_cls_loss: 1.2114 epoch_cnt_loss: 0.1932\n",
      "\n",
      "Eval Epoch: 43 accuracy: 0.5120 epoch_loss: 1.3754 epoch_cls_loss: 1.1809 epoch_cnt_loss: 0.1945\n",
      "\n",
      "Eval Epoch: 44 accuracy: 0.4960 epoch_loss: 1.4018 epoch_cls_loss: 1.2058 epoch_cnt_loss: 0.1959\n",
      "\n",
      "Eval Epoch: 45 accuracy: 0.5100 epoch_loss: 1.4022 epoch_cls_loss: 1.2101 epoch_cnt_loss: 0.1921\n",
      "\n",
      "Eval Epoch: 46 accuracy: 0.5030 epoch_loss: 1.3744 epoch_cls_loss: 1.1959 epoch_cnt_loss: 0.1785\n",
      "\n",
      "Eval Epoch: 47 accuracy: 0.5040 epoch_loss: 1.3541 epoch_cls_loss: 1.1702 epoch_cnt_loss: 0.1838\n",
      "\n",
      "Eval Epoch: 48 accuracy: 0.5170 epoch_loss: 1.3635 epoch_cls_loss: 1.1818 epoch_cnt_loss: 0.1817\n",
      "\n",
      "Early stop at epoch 48. Best eval loss: 1.3465, Best accuracy = 0.497 Optimal epochs: 33\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 1e-3\n",
    "log_interval = 10\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "lambda_cnt = 1.0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "num_workers = min(8, os.cpu_count() or 2)\n",
    "pin = (device.type == \"cuda\")\n",
    "train_kwargs = dict(\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "if num_workers > 0:\n",
    "    train_kwargs[\"prefetch_factor\"] = 4\n",
    "\n",
    "train_augumentation = Augumentation()\n",
    "\n",
    "train_dataset = GSN(root=\".\", transform_relabel=train_augumentation)\n",
    "test_dataset = GSN(root=\".\")\n",
    "\n",
    "train_dataset = Subset(train_dataset, range(0, 9000))\n",
    "test_dataset = Subset(test_dataset, range(9000, 10000))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **train_kwargs)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, **train_kwargs)\n",
    "\n",
    "net = NeuralNetwork(cls_hidden=256).to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "best_eval_loss = float(\"inf\")\n",
    "best_accuracy = 0.0\n",
    "best_state = None\n",
    "bad_epochs = 0\n",
    "bad_epochs_patience = 15\n",
    "optimal_epochs = epochs\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_cls_loss, train_cnt_loss = train_epoch(\n",
    "        net,\n",
    "        device,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        epoch,\n",
    "        log_interval,\n",
    "        lambda_cnt,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    eval_loss, eval_cls_loss, eval_cnt_loss, eval_acc = eval_epoch(\n",
    "        net,\n",
    "        device,\n",
    "        test_loader,\n",
    "        epoch,\n",
    "        lambda_cnt,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    if eval_loss < best_eval_loss:\n",
    "        best_eval_loss = eval_loss\n",
    "        best_accuracy = eval_acc\n",
    "        best_state = {k: v.cpu().clone() for k,v in net.state_dict().items()}\n",
    "        bad_epochs = 0\n",
    "        optimal_epochs = epoch\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "        if bad_epochs >= bad_epochs_patience:\n",
    "            print(f\"Early stop at epoch {epoch}. Best eval loss: {best_eval_loss:.4f}, Best accuracy = {best_accuracy} Optimal epochs: {optimal_epochs}\")\n",
    "            break\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    eval_losses.append(eval_loss)\n",
    "\n",
    "if best_state is not None:\n",
    "    net.load_state_dict(best_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0241b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
