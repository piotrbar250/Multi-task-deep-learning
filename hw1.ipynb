{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077df0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
    "# !unzip data_gsn.zip &> /dev/null\n",
    "# !rm data_gsn.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f3cd8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa94094e490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02cf7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSN(Dataset):\n",
    "    def __init__(self, root, transform=None, transform_relabel=None):\n",
    "        self.data_dir = os.path.join(root, \"data\")\n",
    "        self.transform = transform\n",
    "        self.transform_relabel = transform_relabel\n",
    "\n",
    "        df = pd.read_csv(os.path.join(self.data_dir, \"labels.csv\"))\n",
    "        self.names = df[\"name\"].tolist()\n",
    "        cols = [\"squares\", \"circles\", \"up\", \"right\", \"down\", \"left\"]\n",
    "        self.labels = torch.tensor(df[cols].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        name = self.names[index]\n",
    "        img_path = os.path.join(self.data_dir, name)\n",
    "\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        img = transforms.ToTensor()(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        cnt = self.labels[index]\n",
    "\n",
    "        if self.transform_relabel:\n",
    "            img, cnt = self.transform_relabel(img, cnt)\n",
    "        \n",
    "        cls = self.counts_to_class_id(cnt)\n",
    "\n",
    "        return img, cls, cnt\n",
    "    \n",
    "    def counts_to_class_id(self, counts):\n",
    "        PAIRS = [(i, j) for i in range(6) for j in range(i + 1, 6)]\n",
    "        if isinstance(counts, torch.Tensor):\n",
    "            c = counts.detach().cpu().tolist()\n",
    "        else:\n",
    "            c = list(counts)\n",
    "\n",
    "        nz = [i for i, v in enumerate(c) if v > 0]\n",
    "        if len(nz) != 2:\n",
    "            raise ValueError(f\"Expected exactly 2 nonzero counts, got {len(nz)}: {c}\")\n",
    "        if sum(c) != 10:\n",
    "            raise ValueError(f\"Expected counts to sum to 10, got {sum(c)}: {c}\")\n",
    "\n",
    "        a, b = sorted(nz)\n",
    "        ca = int(c[a])\n",
    "        pair_index = PAIRS.index((a, b))\n",
    "\n",
    "        class_id = pair_index * 9 + (ca - 1)\n",
    "        return class_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5db8493",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augumentation:\n",
    "    def __init__(self, p_hflip=0.5, p_vflip=0.5):\n",
    "        self.p_hflip = p_hflip\n",
    "        self.p_vflip = p_vflip\n",
    "\n",
    "    def __call__(self, img, cnt):\n",
    "        cnt = cnt.clone()\n",
    "\n",
    "        k = torch.randint(0, 4, (1,)).item()\n",
    "        if k > 0:\n",
    "            img = torch.rot90(img, k=-k, dims=[1,2])\n",
    "            dirs = cnt[2:6]\n",
    "            dirs = torch.roll(dirs, shifts=k)\n",
    "            cnt[2:6] = dirs\n",
    "\n",
    "        if torch.rand(1).item() < self.p_hflip:\n",
    "            img = torch.flip(img, dims=[2])\n",
    "            cnt[[3, 5]] = cnt[[5, 3]]\n",
    "        \n",
    "        if torch.rand(1).item() < self.p_vflip:\n",
    "            img = torch.flip(img, dims=[1])\n",
    "            cnt[[2, 4]] = cnt[[4, 2]]\n",
    "\n",
    "        return img, cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6103b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, cls_hidden=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = nn.Sequential(                              # (64, 1, 28, 28)\n",
    "            nn.Conv2d(1, 8, 3, stride=1, padding=1), nn.ReLU(),     # (64, 8, 28 28)  \n",
    "            nn.Conv2d(8, 16, 3, stride=1, padding=1), nn.ReLU(),    # (64, 16, 28, 28)\n",
    "            nn.Conv2d(16, 32, 3, stride=1, padding=1), nn.ReLU(),   # (64, 32, 28, 28)\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1), nn.ReLU(),   # (64, 64, 28, 28)\n",
    "            nn.Flatten(start_dim=1),                                # (64, 64 * 28 * 28)\n",
    "            nn.Linear(64 * 28 * 28, 256), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.head_cls = nn.Sequential(\n",
    "            nn.Linear(256, cls_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(cls_hidden, 135),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.head_cnt = nn.Sequential(\n",
    "            nn.Linear(256, 6)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        cls = self.head_cls(x)  # (64, 135)\n",
    "        cnt = self.head_cnt(x)  # (64, 6)\n",
    "\n",
    "        return cls, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc4775f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    net: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    log_interval: int,\n",
    "    lambda_cnt: float,\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    net.train()\n",
    "    total_loss = 0.0\n",
    "    total_cls_loss = 0.0\n",
    "    total_cnt_loss = 0.0\n",
    "    n_total = 0\n",
    "    \n",
    "    for batch_idx, (img, cls_target, cnt_target) in enumerate(train_loader):\n",
    "        img, cls_target, cnt_target = img.to(device), cls_target.long().to(device), cnt_target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cls_pred, cnt_pred = net(img)\n",
    "        \n",
    "        cls_loss = F.nll_loss(cls_pred, cls_target)\n",
    "        cnt_loss = F.smooth_l1_loss(cnt_pred, cnt_target)\n",
    "        loss = cls_loss + lambda_cnt * cnt_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        B = len(img)\n",
    "\n",
    "        total_loss += loss.item() * B\n",
    "        total_cls_loss += cls_loss.item() * B\n",
    "        total_cnt_loss += cnt_loss.item() * B\n",
    "        n_total += B\n",
    "\n",
    "        if verbose and batch_idx % log_interval == 0:\n",
    "            done = batch_idx * B\n",
    "            total = len(train_loader.dataset)\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch} [{done}/{total} images ({done / total:.0%})]\\t\"\n",
    "                + f\"Loss: {loss.item():.6f}\"\n",
    "            )\n",
    "\n",
    "    epoch_loss = total_loss / n_total\n",
    "    epoch_cls_loss = total_cls_loss / n_total\n",
    "    epoch_cnt_loss = total_cnt_loss / n_total\n",
    "    return epoch_loss, epoch_cls_loss, epoch_cnt_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "437bc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(\n",
    "    net: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    epoch: int,\n",
    "    lambda_cnt: float,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    net.eval()\n",
    "    total_loss = 0.0\n",
    "    total_cls_loss = 0.0\n",
    "    total_cnt_loss = 0.0\n",
    "    n_total = 0\n",
    "    correct = 0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, cls_target, cnt_target in test_loader:\n",
    "            img, cls_target, cnt_target = img.to(device), cls_target.long().to(device), cnt_target.to(device)\n",
    "\n",
    "            cls_pred, cnt_pred = net(img)\n",
    "            cls_loss = F.nll_loss(cls_pred, cls_target)\n",
    "            cnt_loss = F.smooth_l1_loss(cnt_pred, cnt_target)\n",
    "            loss = cls_loss + lambda_cnt * cnt_loss\n",
    "\n",
    "            total_loss += loss.item() * len(img)\n",
    "            total_cls_loss += cls_loss.item() * len(img)\n",
    "            total_cnt_loss += cnt_loss.item() * len(img)\n",
    "            n_total += len(img)\n",
    "\n",
    "            pred = cls_pred.argmax(dim=1)\n",
    "            correct += (pred == cls_target).sum().item()\n",
    "\n",
    "    epoch_loss = total_loss / n_total\n",
    "    epoch_cls_loss = total_cls_loss / n_total\n",
    "    epoch_cnt_loss = total_cnt_loss / n_total\n",
    "    epoch_acc = correct / n_total\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Eval Epoch: {epoch} accuracy: {epoch_acc:.4f} epoch_loss: {epoch_loss:.4f} epoch_cls_loss: {epoch_cls_loss:.4f} epoch_cnt_loss: {epoch_cnt_loss:.4f}\\n\")\n",
    "    return epoch_loss, epoch_cls_loss, epoch_cnt_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da83da5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Epoch: 1 accuracy: 0.0450 epoch_loss: 4.9313 epoch_cls_loss: 3.8612 epoch_cnt_loss: 1.0701\n",
      "\n",
      "Eval Epoch: 2 accuracy: 0.1590 epoch_loss: 3.4784 epoch_cls_loss: 2.7803 epoch_cnt_loss: 0.6981\n",
      "\n",
      "Eval Epoch: 3 accuracy: 0.2430 epoch_loss: 2.6774 epoch_cls_loss: 2.1726 epoch_cnt_loss: 0.5048\n",
      "\n",
      "Eval Epoch: 4 accuracy: 0.2770 epoch_loss: 2.3417 epoch_cls_loss: 1.9267 epoch_cnt_loss: 0.4150\n",
      "\n",
      "Eval Epoch: 5 accuracy: 0.3100 epoch_loss: 2.1067 epoch_cls_loss: 1.7406 epoch_cnt_loss: 0.3661\n",
      "\n",
      "Eval Epoch: 6 accuracy: 0.3120 epoch_loss: 2.1322 epoch_cls_loss: 1.7499 epoch_cnt_loss: 0.3823\n",
      "\n",
      "Eval Epoch: 7 accuracy: 0.3530 epoch_loss: 1.8476 epoch_cls_loss: 1.5652 epoch_cnt_loss: 0.2824\n",
      "\n",
      "Eval Epoch: 8 accuracy: 0.3630 epoch_loss: 1.8182 epoch_cls_loss: 1.5291 epoch_cnt_loss: 0.2891\n",
      "\n",
      "Eval Epoch: 9 accuracy: 0.3650 epoch_loss: 1.7848 epoch_cls_loss: 1.5129 epoch_cnt_loss: 0.2719\n",
      "\n",
      "Eval Epoch: 10 accuracy: 0.3690 epoch_loss: 1.7745 epoch_cls_loss: 1.4975 epoch_cnt_loss: 0.2770\n",
      "\n",
      "Eval Epoch: 11 accuracy: 0.4000 epoch_loss: 1.6358 epoch_cls_loss: 1.3918 epoch_cnt_loss: 0.2440\n",
      "\n",
      "Eval Epoch: 12 accuracy: 0.4090 epoch_loss: 1.6764 epoch_cls_loss: 1.4239 epoch_cnt_loss: 0.2525\n",
      "\n",
      "Eval Epoch: 13 accuracy: 0.4210 epoch_loss: 1.6180 epoch_cls_loss: 1.3719 epoch_cnt_loss: 0.2461\n",
      "\n",
      "Eval Epoch: 14 accuracy: 0.4070 epoch_loss: 1.5961 epoch_cls_loss: 1.3676 epoch_cnt_loss: 0.2285\n",
      "\n",
      "Eval Epoch: 15 accuracy: 0.3990 epoch_loss: 1.6475 epoch_cls_loss: 1.3934 epoch_cnt_loss: 0.2541\n",
      "\n",
      "Eval Epoch: 16 accuracy: 0.4170 epoch_loss: 1.5976 epoch_cls_loss: 1.3608 epoch_cnt_loss: 0.2368\n",
      "\n",
      "Eval Epoch: 17 accuracy: 0.4480 epoch_loss: 1.5695 epoch_cls_loss: 1.3376 epoch_cnt_loss: 0.2319\n",
      "\n",
      "Eval Epoch: 18 accuracy: 0.4390 epoch_loss: 1.5258 epoch_cls_loss: 1.3067 epoch_cnt_loss: 0.2191\n",
      "\n",
      "Eval Epoch: 19 accuracy: 0.4430 epoch_loss: 1.4956 epoch_cls_loss: 1.2717 epoch_cnt_loss: 0.2239\n",
      "\n",
      "Eval Epoch: 20 accuracy: 0.4590 epoch_loss: 1.4365 epoch_cls_loss: 1.2290 epoch_cnt_loss: 0.2075\n",
      "\n",
      "Eval Epoch: 21 accuracy: 0.4820 epoch_loss: 1.4146 epoch_cls_loss: 1.2123 epoch_cnt_loss: 0.2023\n",
      "\n",
      "Eval Epoch: 22 accuracy: 0.4740 epoch_loss: 1.4259 epoch_cls_loss: 1.2242 epoch_cnt_loss: 0.2017\n",
      "\n",
      "Eval Epoch: 23 accuracy: 0.4530 epoch_loss: 1.4795 epoch_cls_loss: 1.2695 epoch_cnt_loss: 0.2100\n",
      "\n",
      "Eval Epoch: 24 accuracy: 0.4840 epoch_loss: 1.3908 epoch_cls_loss: 1.1962 epoch_cnt_loss: 0.1946\n",
      "\n",
      "Eval Epoch: 25 accuracy: 0.4720 epoch_loss: 1.4430 epoch_cls_loss: 1.2332 epoch_cnt_loss: 0.2098\n",
      "\n",
      "Eval Epoch: 26 accuracy: 0.4670 epoch_loss: 1.4343 epoch_cls_loss: 1.2318 epoch_cnt_loss: 0.2025\n",
      "\n",
      "Eval Epoch: 27 accuracy: 0.4470 epoch_loss: 1.5074 epoch_cls_loss: 1.2919 epoch_cnt_loss: 0.2156\n",
      "\n",
      "Eval Epoch: 28 accuracy: 0.4920 epoch_loss: 1.4405 epoch_cls_loss: 1.2389 epoch_cnt_loss: 0.2016\n",
      "\n",
      "Eval Epoch: 29 accuracy: 0.4860 epoch_loss: 1.4333 epoch_cls_loss: 1.2369 epoch_cnt_loss: 0.1965\n",
      "\n",
      "Eval Epoch: 30 accuracy: 0.5040 epoch_loss: 1.3595 epoch_cls_loss: 1.1756 epoch_cnt_loss: 0.1840\n",
      "\n",
      "Eval Epoch: 31 accuracy: 0.4630 epoch_loss: 1.4668 epoch_cls_loss: 1.2515 epoch_cnt_loss: 0.2153\n",
      "\n",
      "Eval Epoch: 32 accuracy: 0.4840 epoch_loss: 1.3972 epoch_cls_loss: 1.2056 epoch_cnt_loss: 0.1916\n",
      "\n",
      "Eval Epoch: 33 accuracy: 0.5140 epoch_loss: 1.4223 epoch_cls_loss: 1.2364 epoch_cnt_loss: 0.1859\n",
      "\n",
      "Eval Epoch: 34 accuracy: 0.4960 epoch_loss: 1.4448 epoch_cls_loss: 1.2504 epoch_cnt_loss: 0.1944\n",
      "\n",
      "Eval Epoch: 35 accuracy: 0.5070 epoch_loss: 1.4061 epoch_cls_loss: 1.2217 epoch_cnt_loss: 0.1843\n",
      "\n",
      "Eval Epoch: 36 accuracy: 0.4580 epoch_loss: 1.5747 epoch_cls_loss: 1.3681 epoch_cnt_loss: 0.2066\n",
      "\n",
      "Eval Epoch: 37 accuracy: 0.5050 epoch_loss: 1.3914 epoch_cls_loss: 1.2076 epoch_cnt_loss: 0.1838\n",
      "\n",
      "Eval Epoch: 38 accuracy: 0.4960 epoch_loss: 1.4455 epoch_cls_loss: 1.2600 epoch_cnt_loss: 0.1854\n",
      "\n",
      "Eval Epoch: 39 accuracy: 0.4860 epoch_loss: 1.5200 epoch_cls_loss: 1.3140 epoch_cnt_loss: 0.2060\n",
      "\n",
      "Eval Epoch: 40 accuracy: 0.4650 epoch_loss: 1.5463 epoch_cls_loss: 1.3476 epoch_cnt_loss: 0.1987\n",
      "\n",
      "Early stop at epoch 40. Best eval loss: 1.3595, Best accuracy = 0.504 Optimal epochs: 30\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 1e-3\n",
    "log_interval = 10\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "lambda_cnt = 1.0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "num_workers = min(8, os.cpu_count() or 2)\n",
    "pin = (device.type == \"cuda\")\n",
    "train_kwargs = dict(\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "if num_workers > 0:\n",
    "    train_kwargs[\"prefetch_factor\"] = 4\n",
    "\n",
    "train_augumentation = Augumentation()\n",
    "\n",
    "train_dataset = GSN(root=\".\", transform_relabel=train_augumentation)\n",
    "test_dataset = GSN(root=\".\")\n",
    "\n",
    "train_dataset = Subset(train_dataset, range(0, 9000))\n",
    "test_dataset = Subset(test_dataset, range(9000, 10000))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **train_kwargs)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, **train_kwargs)\n",
    "\n",
    "net = NeuralNetwork(cls_hidden=256, dropout=0.3).to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "best_eval_loss = float(\"inf\")\n",
    "best_accuracy = 0.0\n",
    "best_state = None\n",
    "bad_epochs = 0\n",
    "bad_epochs_patience = 10\n",
    "optimal_epochs = epochs\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_cls_loss, train_cnt_loss = train_epoch(\n",
    "        net,\n",
    "        device,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        epoch,\n",
    "        log_interval,\n",
    "        lambda_cnt,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    eval_loss, eval_cls_loss, eval_cnt_loss, eval_acc = eval_epoch(\n",
    "        net,\n",
    "        device,\n",
    "        test_loader,\n",
    "        epoch,\n",
    "        lambda_cnt,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    if eval_loss < best_eval_loss:\n",
    "        best_eval_loss = eval_loss\n",
    "        best_accuracy = eval_acc\n",
    "        best_state = {k: v.cpu().clone() for k,v in net.state_dict().items()}\n",
    "        bad_epochs = 0\n",
    "        optimal_epochs = epoch\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "        if bad_epochs >= bad_epochs_patience:\n",
    "            print(f\"Early stop at epoch {epoch}. Best eval loss: {best_eval_loss:.4f}, Best accuracy = {best_accuracy} Optimal epochs: {optimal_epochs}\")\n",
    "            break\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    eval_losses.append(eval_loss)\n",
    "\n",
    "if best_state is not None:\n",
    "    net.load_state_dict(best_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0241b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = \"0512-net.pt\"\n",
    "# torch.save(net.state_dict(), save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
